{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nsub = pd.read_csv('../input/ccf2021bankdefault/submit_example.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-18T05:15:16.922576Z","iopub.execute_input":"2021-11-18T05:15:16.923562Z","iopub.status.idle":"2021-11-18T05:15:16.971052Z","shell.execute_reply.started":"2021-11-18T05:15:16.923407Z","shell.execute_reply":"2021-11-18T05:15:16.970083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"本赛题要求利用已有的与目标客群稍有差异的另一批信贷数据，辅助目标业务风控模型的创建，两者数据集之间存在大量相同的字段和极少的共同用户。此处希望大家可以利用迁移学习捕捉不同业务中用户基本信息与违约行为之间的关联，帮助实现对新业务的用户违约预测。","metadata":{}},{"cell_type":"code","source":"!wget https://awscdn.datafountain.cn/cometition_data2/Files/BDCI2021/530ZhongYuan/train_dataset.zip","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:16.975281Z","iopub.execute_input":"2021-11-18T05:15:16.975552Z","iopub.status.idle":"2021-11-18T05:15:25.742862Z","shell.execute_reply.started":"2021-11-18T05:15:16.975519Z","shell.execute_reply":"2021-11-18T05:15:25.741706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip ./train_dataset.zip","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:25.744714Z","iopub.execute_input":"2021-11-18T05:15:25.745353Z","iopub.status.idle":"2021-11-18T05:15:28.035257Z","shell.execute_reply.started":"2021-11-18T05:15:25.745289Z","shell.execute_reply":"2021-11-18T05:15:28.034224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://awscdn.datafountain.cn/cometition_data2/Files/BDCI2021/530ZhongYuan/test_public.csv","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:28.038338Z","iopub.execute_input":"2021-11-18T05:15:28.038735Z","iopub.status.idle":"2021-11-18T05:15:31.35908Z","shell.execute_reply.started":"2021-11-18T05:15:28.038663Z","shell.execute_reply":"2021-11-18T05:15:31.358017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 6666","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:31.360962Z","iopub.execute_input":"2021-11-18T05:15:31.36125Z","iopub.status.idle":"2021-11-18T05:15:31.365426Z","shell.execute_reply.started":"2021-11-18T05:15:31.361214Z","shell.execute_reply":"2021-11-18T05:15:31.36446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 1:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport re\nimport pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom dateutil.relativedelta import relativedelta\ntrain_data = pd.read_csv('./train_public.csv')\nsubmit_example = pd.read_csv('../input/ccf2021bankdefault/submit_example.csv')\ntest_public = pd.read_csv('./test_public.csv')\ntrain_inte = pd.read_csv('./train_internet.csv')\n\ndef train_model(data_, test_, y_, folds_):\n    oof_preds = np.zeros(data_.shape[0])\n    sub_preds = np.zeros(test_.shape[0])\n    feature_importance_df = pd.DataFrame()\n    #feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault','policy_code','del_in_18month'] ]\n    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n        cat_feats={'industry','employer_type'}\n        clf = LGBMClassifier(\n            n_estimators=4000,\n            learning_rate=0.08,#0.07\n            num_leaves=2**5+1,\n            colsample_bytree=.65,\n            subsample=.9,\n            max_depth=5,#5\n            #max_bin=250,\n            reg_alpha=.3,\n            reg_lambda=.3,\n            min_split_gain=.01,\n            min_child_weight=2,\n            silent=-1,\n            verbose=-1,\n        )\n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)],#categorical_feature=cat_feats,\n                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n               )\n\n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_[feats], num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n        gc.collect()  \n    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n    \n    test_['isDefault'] = sub_preds\n\n    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\ndef display_importances(feature_importance_df_):\n    # Plot feature importances\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(8,10))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')\ndef workYearDIc(x):\n    if str(x)=='nan':\n        return -1\n    x = x.replace('< 1','0')\n    return int(re.search('(\\d+)', x).group())\ndef findDig(val):\n    fd = re.search('(\\d+-)', val)\n    if fd is None:\n        return '1-'+val\n    return val + '-01'\nclass_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n}\n#public+inte 双表\n\n#work_year\ntrain_data['work_year'] = train_data['work_year'].map(workYearDIc)\ntest_public['work_year'] = test_public['work_year'].map(workYearDIc)\ntrain_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n\n#class\ntrain_data['class'] = train_data['class'].map(class_dict)\ntest_public['class'] = test_public['class'].map(class_dict)\ntrain_inte['class'] = train_inte['class'].map(class_dict)\n\n#earlies_credit_mon\ntrain_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\ntest_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\ntrain_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'].map(findDig))\n\n#earlies_credit_mon\ntimeMax = pd.to_datetime('1-Dec-21')\ntrain_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \ntest_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\ntrain_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n\n#issue_date\ntrain_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\ntest_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\ntrain_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n\ntrain_data['issue_date_month'] = train_data['issue_date'].dt.month\ntest_public['issue_date_month'] = test_public['issue_date'].dt.month\ntrain_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n\ntrain_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\ntest_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\ntrain_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n\ntrain_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\ntest_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\ntrain_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n\ntrain_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\ntest_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\ntrain_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n\n#特征构造\n\n#月收入\n# train_data['monthly_income'] = train_data['monthly_payment'] / train_data['debt_loan_ratio'] *100\n# test_public['monthly_income'] = test_public['monthly_payment'] / test_public['debt_loan_ratio'] *100\n# train_inte['monthly_income'] = train_inte['monthly_payment'] / train_inte['debt_loan_ratio'] *100\n\ntrain_data['post_code_to_mean_interst'] = train_data.groupby(['post_code'])['interest'].transform('mean')\ntest_public['post_code_to_mean_interst'] = test_public.groupby(['post_code'])['interest'].transform('mean')\ntrain_inte['post_code_to_mean_interst'] = train_inte.groupby(['post_code'])['interest'].transform('mean')\n\ntrain_data['post_code_cnt'] = train_data['post_code'].map(train_data['post_code'].value_counts())\ntrain_data.loc[train_data['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntest_public['post_code_cnt'] = test_public['post_code'].map(test_public['post_code'].value_counts())\ntest_public.loc[test_public['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntrain_inte['post_code_cnt'] = train_inte['post_code'].map(train_inte['post_code'].value_counts())\ntrain_inte.loc[train_inte['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\n\n\ndel train_data['post_code_cnt']\ndel test_public['post_code_cnt']\ndel train_inte['post_code_cnt']\n#类别型特征编码\nfrom sklearn.preprocessing import LabelEncoder\ncat_cols = ['employer_type', 'industry']\nfor col in cat_cols:\n    lbl = LabelEncoder().fit(train_data[col])\n    train_data[col] = lbl.transform(train_data[col])\n    test_public[col] = lbl.transform(test_public[col])\n    train_inte[col] = lbl.transform(train_inte[col])\ncol_to_drop = ['issue_date', 'earlies_credit_mon']\ntrain_data = train_data.drop(col_to_drop, axis=1)\ntest_public = test_public.drop(col_to_drop, axis=1 )\ntrain_inte = train_inte.drop(col_to_drop, axis=1 )\n#取train_data和train_inte特征交集，train_inte将不包含的特征使用nan值填补\ntr_cols = set(train_data.columns)\nsame_col = list(tr_cols.intersection(set(train_inte.columns)))\ntrain_inteSame = train_inte[same_col].copy()\nInte_add_cos = list(tr_cols.difference(set(same_col)))\nfor col in Inte_add_cos:\n    train_inteSame[col] = np.nan\n#伪标签学习      以train_data特征训练模型->预测train_inteSame\ny = train_data['isDefault']\ntrain_data.shape\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:31.367045Z","iopub.execute_input":"2021-11-18T05:15:31.367275Z","iopub.status.idle":"2021-11-18T05:15:54.042799Z","shell.execute_reply.started":"2021-11-18T05:15:31.367249Z","shell.execute_reply":"2021-11-18T05:15:54.041802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#roc_auc_score\nfrom sklearn.metrics import roc_auc_score\nIntePre['isDef'] = train_inte['is_default']\nroc_auc_score(IntePre['isDef'],IntePre.isDefault)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:54.044286Z","iopub.execute_input":"2021-11-18T05:15:54.04456Z","iopub.status.idle":"2021-11-18T05:15:54.3807Z","shell.execute_reply.started":"2021-11-18T05:15:54.044526Z","shell.execute_reply":"2021-11-18T05:15:54.379964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 2：","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# train_data = pd.read_csv('./train_public.csv')\n# test_data = pd.read_csv('./test_public.csv')\n# sub=pd.read_csv(\"nn2.csv\")\n# sub=sub.rename(columns={'id': 'loan_id'})\n# thr = sub['isDefault'].quantile(0.9)\n# sub.loc[sub['isDefault']>thr,'isDefault'] = 1\n# sub.loc[sub['isDefault']<0.5,'isDefault'] = 0\n# nw_sub=sub[(sub['isDefault']==1)|(sub['isDefault']==0)]\n# nw_test_data=test_data.merge(nw_sub,on='loan_id',how='inner')\n# nw_train_data = pd.concat([train_data,nw_test_data]).reset_index(drop=True)\n# nw_train_data.to_csv(\"./nw_train_public.csv\",index=0)\n## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\nInteId = IntePre.loc[IntePre.isDefault<0.05, 'loan_id'].tolist()\n#新增来源域分类特征\ntrain_data['dataSourse'] = 1\ntest_public['dataSourse'] = 1\ntrain_inteSame['dataSourse'] = 0\ntrain_inteSame['isDefault'] = train_inte['is_default']\nuse_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\n#连接表\ndata = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:54.381987Z","iopub.execute_input":"2021-11-18T05:15:54.382225Z","iopub.status.idle":"2021-11-18T05:15:54.762908Z","shell.execute_reply.started":"2021-11-18T05:15:54.382198Z","shell.execute_reply":"2021-11-18T05:15:54.761894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IntePre.isDefault\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of Default values IntePre\")\nsns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\nplt.legend();plt.show()\ntrain = data[data['isDefault'].notna()]\ntest  = data[data['isDefault'].isna()]\n\ndel data\ndel train_data,test_public","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:54.76429Z","iopub.execute_input":"2021-11-18T05:15:54.764535Z","iopub.status.idle":"2021-11-18T05:15:59.255452Z","shell.execute_reply.started":"2021-11-18T05:15:54.764507Z","shell.execute_reply":"2021-11-18T05:15:59.25466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['isDefault']\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, test_preds, importances = train_model(train, test, y, folds)\ntest_preds.rename({'loan_id': 'id'}, axis=1)[['id', 'isDefault']].to_csv('./nn2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:15:59.258344Z","iopub.execute_input":"2021-11-18T05:15:59.258582Z","iopub.status.idle":"2021-11-18T05:16:04.356254Z","shell.execute_reply.started":"2021-11-18T05:15:59.258553Z","shell.execute_reply":"2021-11-18T05:16:04.355255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## step 3:","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('./train_public.csv')\ntest_data = pd.read_csv('./test_public.csv')\nsub=pd.read_csv(\"nn2.csv\")\nsub=sub.rename(columns={'id': 'loan_id'})","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:04.357789Z","iopub.execute_input":"2021-11-18T05:16:04.358025Z","iopub.status.idle":"2021-11-18T05:16:04.461315Z","shell.execute_reply.started":"2021-11-18T05:16:04.357995Z","shell.execute_reply":"2021-11-18T05:16:04.460454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.loc[sub['isDefault']<0.5,'isDefault'] = 0\nnw_sub=sub[(sub['isDefault']==0)]\nnw_test_data=test_data.merge(nw_sub,on='loan_id',how='inner')\nnw_train_data = pd.concat([train_data,nw_test_data]).reset_index(drop=True)\nnw_train_data.to_csv(\"./nw_train_public.csv\",index=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:04.462828Z","iopub.execute_input":"2021-11-18T05:16:04.463066Z","iopub.status.idle":"2021-11-18T05:16:04.943049Z","shell.execute_reply.started":"2021-11-18T05:16:04.463037Z","shell.execute_reply":"2021-11-18T05:16:04.942203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport re\nimport pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom dateutil.relativedelta import relativedelta\ntrain_data = pd.read_csv('./nw_train_public.csv')\nsubmit_example = pd.read_csv('../input/ccf2021bankdefault/submit_example.csv')\ntest_public = pd.read_csv('./test_public.csv')\ntrain_inte = pd.read_csv('./train_internet.csv')\n\npd.set_option('max_columns', None)\npd.set_option('max_rows', 200)\npd.set_option('float_format', lambda x: '%.3f' % x)\ndef train_model(data_, test_, y_, folds_):\n    oof_preds = np.zeros(data_.shape[0])\n    sub_preds = np.zeros(test_.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n        clf = LGBMClassifier(\n            n_estimators=4000,\n            learning_rate=0.08,#0.07\n            num_leaves=2**5+1,\n            colsample_bytree=.65,\n            subsample=.9,\n            max_depth=5,#5\n            max_bin=250,\n            reg_alpha=.3,\n            reg_lambda=.3,\n            min_split_gain=.01,\n            min_child_weight=2,\n            silent=-1,\n            verbose=-1,\n        )\n        cat_feats={'industry','employer_type'}\n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)],#categorical_feature=cat_feats,\n                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n               )\n\n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n        sub_preds += clf.predict_proba(test_[feats], num_iteration=clf.best_iteration_)[:, 1] / folds_.n_splits\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n        gc.collect()\n        \n    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n    \n    test_['isDefault'] = sub_preds\n\n    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\ndef workYearDIc(x):\n    if str(x)=='nan':\n        return -1\n    x = x.replace('< 1','0')\n    return int(re.search('(\\d+)', x).group())\n\ndef findDig(val):\n    fd = re.search('(\\d+-)', val)\n    if fd is None:\n        return '1-'+val\n    return val + '-01'\n\n\nclass_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n}\n\n#public+inte 双表\n\n#work_year\ntrain_data['work_year'] = train_data['work_year'].map(workYearDIc)\ntest_public['work_year'] = test_public['work_year'].map(workYearDIc)\ntrain_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n\n#class\ntrain_data['class'] = train_data['class'].map(class_dict)\ntest_public['class'] = test_public['class'].map(class_dict)\ntrain_inte['class'] = train_inte['class'].map(class_dict)\n\n#earlies_credit_mon\ntrain_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\ntest_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\ntrain_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'].map(findDig))\n\n#earlies_credit_mon\ntimeMax = pd.to_datetime('1-Dec-21')\ntrain_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \ntest_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\ntrain_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n\n#issue_date\ntrain_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\ntest_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\ntrain_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n\ntrain_data['issue_date_month'] = train_data['issue_date'].dt.month\ntest_public['issue_date_month'] = test_public['issue_date'].dt.month\ntrain_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n\ntrain_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\ntest_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\ntrain_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n\ntrain_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\ntest_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\ntrain_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n\ntrain_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\ntest_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\ntrain_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n\n#特征构造\n\n#月收入\n# train_data['monthly_income'] = train_data['monthly_payment'] / train_data['debt_loan_ratio'] *100\n# test_public['monthly_income'] = test_public['monthly_payment'] / test_public['debt_loan_ratio'] *100\n# train_inte['monthly_income'] = train_inte['monthly_payment'] / train_inte['debt_loan_ratio'] *100\n\ntrain_data['post_code_to_mean_interst'] = train_data.groupby(['post_code'])['interest'].transform('mean')\ntest_public['post_code_to_mean_interst'] = test_public.groupby(['post_code'])['interest'].transform('mean')\ntrain_inte['post_code_to_mean_interst'] = train_inte.groupby(['post_code'])['interest'].transform('mean')\n\ntrain_data['post_code_cnt'] = train_data['post_code'].map(train_data['post_code'].value_counts())\ntrain_data.loc[train_data['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntest_public['post_code_cnt'] = test_public['post_code'].map(test_public['post_code'].value_counts())\ntest_public.loc[test_public['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntrain_inte['post_code_cnt'] = train_inte['post_code'].map(train_inte['post_code'].value_counts())\ntrain_inte.loc[train_inte['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\n\ndel train_data['post_code_cnt']\ndel test_public['post_code_cnt']\ndel train_inte['post_code_cnt']\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in cat_cols:\n    cat_cols = ['employer_type', 'industry']\n    lbl = LabelEncoder().fit(train_data[col])\n    train_data[col] = lbl.transform(train_data[col])\n    test_public[col] = lbl.transform(test_public[col])\n    \n    #Internet处理\n    train_inte[col] = lbl.transform(train_inte[col])\n    \ncol_to_drop = ['issue_date', 'earlies_credit_mon']\ntrain_data = train_data.drop(col_to_drop, axis=1)\ntest_public = test_public.drop(col_to_drop, axis=1 )\ntrain_inte = train_inte.drop(col_to_drop, axis=1 )\n\ntr_cols = set(train_data.columns)\nsame_col = list(tr_cols.intersection(set(train_inte.columns)))\ntrain_inteSame = train_inte[same_col].copy()\n\nInte_add_cos = list(tr_cols.difference(set(same_col)))\nfor col in Inte_add_cos:\n    train_inteSame[col] = np.nan\n\ny = train_data['isDefault']\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)\n\nIntePre['isDef'] = train_inte['is_default']\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(IntePre['isDef'],IntePre.isDefault)\n## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\nInteId = IntePre.loc[IntePre.isDefault<0.5, 'loan_id'].tolist()\n\ntrain_data['dataSourse'] = 1\ntest_public['dataSourse'] = 1\ntrain_inteSame['dataSourse'] = 0\ntrain_inteSame['isDefault'] = train_inte['is_default']\nuse_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\ndata = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)\n\n####****新增数据######\n# interest#当前贷款利率\n# debt_loan_ratio#债务收入比\n\nfor method in ['mean','std','sum','median']:\n    for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n        data[f'label_{method}_'+str(col)] = data.groupby(col)['isDefault'].transform(method)\n        \n# for method in ['mean', 'max', 'min', 'std','sum','median']:\n#     for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n#         data[f'interest_{method}_'+str(col)] = data.groupby(col)['known_outstanding_loan'].transform(method)\n# for method in ['mean', 'max', 'min', 'std','sum','median']:\n#     for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n#         data[f'debt_loan_ratio_{method}_'+str(col)] = data.groupby(col)['debt_loan_ratio'].transform(method)\n\n\n# data = data.replace([-np.inf,np.inf],np.NaN)\n# Kdf = data[['known_outstanding_loan','debt_loan_ratio','monthly_payment','f2','total_loan','f4','f3','interest']]\n# Kdf = Kdf.fillna(0)\n# kmeans = KMeans(3,random_state=2021)\n# kmeans.fit(Kdf)\n# identified_clusters = kmeans.predict(Kdf)\n# data['k_cluster'] = identified_clusters\n\n# for method in ['mean', 'max', 'min', 'std']:\n#     for col in ['known_outstanding_loan','debt_loan_ratio','monthly_payment','f2','total_loan','f4','f3','interest']:\n#         data[f'k_cluster_{method}_'+str(col)] = data.groupby('k_cluster')[col].transform(method)\n        \nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of Default values IntePre\")\nsns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\n# sns.distplot(train_inte[col],color=\"red\", kde=True,bins=120, label='train_inte')\nplt.legend();plt.show()\ntrain = data[data['isDefault'].notna()]\ntest  = data[data['isDefault'].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:04.944682Z","iopub.execute_input":"2021-11-18T05:16:04.944951Z","iopub.status.idle":"2021-11-18T05:16:35.179615Z","shell.execute_reply.started":"2021-11-18T05:16:04.944924Z","shell.execute_reply":"2021-11-18T05:16:35.178781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_importances(importances)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:35.181231Z","iopub.execute_input":"2021-11-18T05:16:35.181603Z","iopub.status.idle":"2021-11-18T05:16:37.250546Z","shell.execute_reply.started":"2021-11-18T05:16:35.181571Z","shell.execute_reply":"2021-11-18T05:16:37.249582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb_model_0\nfeats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault']]\n#lgb_model_1\n#feats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault','f1']]\nx_train = train[feats]\ny_train = train['isDefault']\nx_test = test[feats]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:37.252011Z","iopub.execute_input":"2021-11-18T05:16:37.252307Z","iopub.status.idle":"2021-11-18T05:16:37.510997Z","shell.execute_reply.started":"2021-11-18T05:16:37.252266Z","shell.execute_reply":"2021-11-18T05:16:37.510301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train = np.zeros((x_train.shape[0], 4))\ndataset_blend_test = np.zeros((x_test.shape[0], 4))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:37.512242Z","iopub.execute_input":"2021-11-18T05:16:37.513016Z","iopub.status.idle":"2021-11-18T05:16:37.527854Z","shell.execute_reply.started":"2021-11-18T05:16:37.51297Z","shell.execute_reply":"2021-11-18T05:16:37.526836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lgb_model_0(X_train, y_train, X_test, y_test=None):\n    #X_train = pd.DataFrame(X_train)\n    #X_test = pd.DataFrame(X_test)\n    folds_ = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n    cat_feats={'industry','employer_type'}\n    dataset_blend_test_0 = np.zeros((X_test.shape[0], 5))\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(X_train)):\n        trn_x, trn_y = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        clf = LGBMClassifier(\n            n_estimators=4000,\n            learning_rate=0.08,#0.07\n            num_leaves=2**5+1,\n            colsample_bytree=.65,\n            subsample=.9,\n            max_depth=5,#5\n            #max_bin=250,\n            reg_alpha=.3,\n            reg_lambda=.3,\n            min_split_gain=.01,\n            min_child_weight=2,\n            silent=-1,\n            verbose=-1,\n        )\n        \n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)], categorical_feature=cat_feats,\n                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n               )\n        \n        vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n\n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n        minmin= min(oof_preds[val_idx])\n        maxmax= max(oof_preds[val_idx])\n        dataset_blend_train[val_idx,0]= vfunc(oof_preds[val_idx])\n        sub_preds = clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:, 1]\n        #dataset_blend_test_0[:,n_fold] = sub_preds\n        minmin= min(sub_preds)\n        maxmax= max(sub_preds)\n        dataset_blend_test_0[:,n_fold] = vfunc(sub_preds)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    print('Full AUC score %.6f' % roc_auc_score(y_train, oof_preds)) \n    return dataset_blend_train,dataset_blend_test_0","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:37.532027Z","iopub.execute_input":"2021-11-18T05:16:37.532279Z","iopub.status.idle":"2021-11-18T05:16:37.550304Z","shell.execute_reply.started":"2021-11-18T05:16:37.532249Z","shell.execute_reply":"2021-11-18T05:16:37.549235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train,dataset_blend_test_0 = lgb_model_0(x_train, y_train, x_test)\ndataset_blend_test[:,0]= dataset_blend_test_0.mean(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:16:37.551379Z","iopub.execute_input":"2021-11-18T05:16:37.552259Z","iopub.status.idle":"2021-11-18T05:24:34.016117Z","shell.execute_reply.started":"2021-11-18T05:16:37.552212Z","shell.execute_reply":"2021-11-18T05:24:34.015042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['isDefault'] = dataset_blend_test[:,0]\nthr1 = test['isDefault'].quantile(0.9)\nthr2 = test['isDefault'].quantile(0.1)\ntest['isDefault'][dataset_blend_test[:,0]>thr1]=1\ntest['isDefault'][dataset_blend_test[:,0]<thr2]=0\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:34.018865Z","iopub.execute_input":"2021-11-18T05:24:34.019413Z","iopub.status.idle":"2021-11-18T05:24:34.049439Z","shell.execute_reply.started":"2021-11-18T05:24:34.019354Z","shell.execute_reply":"2021-11-18T05:24:34.048786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dat = test[(test['isDefault']==1) | (test['isDefault']==0)]\ntrain = pd.concat([train,new_dat])","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:34.050788Z","iopub.execute_input":"2021-11-18T05:24:34.05118Z","iopub.status.idle":"2021-11-18T05:24:34.455559Z","shell.execute_reply.started":"2021-11-18T05:24:34.051147Z","shell.execute_reply":"2021-11-18T05:24:34.454608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['isDefault'].quantile(0.9)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:34.456864Z","iopub.execute_input":"2021-11-18T05:24:34.457148Z","iopub.status.idle":"2021-11-18T05:24:34.46607Z","shell.execute_reply.started":"2021-11-18T05:24:34.457112Z","shell.execute_reply":"2021-11-18T05:24:34.465214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb_model_0\nfeats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault']]\n#lgb_model_1\n#feats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault','f1']]\nx_train = train[feats]\ny_train = train['isDefault']\nx_test = test[feats]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:34.467601Z","iopub.execute_input":"2021-11-18T05:24:34.467849Z","iopub.status.idle":"2021-11-18T05:24:35.146077Z","shell.execute_reply.started":"2021-11-18T05:24:34.467821Z","shell.execute_reply":"2021-11-18T05:24:35.145071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train = np.zeros((x_train.shape[0], 4))\ndataset_blend_test = np.zeros((x_test.shape[0], 4))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:35.147584Z","iopub.execute_input":"2021-11-18T05:24:35.147885Z","iopub.status.idle":"2021-11-18T05:24:35.159042Z","shell.execute_reply.started":"2021-11-18T05:24:35.147853Z","shell.execute_reply":"2021-11-18T05:24:35.158132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lgb_model_1(X_train, y_train, X_test, y_test=None):\n    #X_train = pd.DataFrame(X_train)\n    #X_test = pd.DataFrame(X_test)\n    folds_ = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(x_train.shape[0])\n    sub_preds = np.zeros(x_test.shape[0])\n    cat_feats={'industry','employer_type'}\n    dataset_blend_test_0 = np.zeros((x_test.shape[0], 5))\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(x_train)):\n        trn_x, trn_y = x_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        val_x, val_y = x_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        clf = LGBMClassifier(\n            n_estimators=4000,\n            learning_rate=0.08,#0.07\n            num_leaves=2**5+1,\n            colsample_bytree=.65,\n            subsample=.9,\n            max_depth=5,#5\n            #max_bin=250,\n            reg_alpha=.3,\n            reg_lambda=.3,\n            min_split_gain=.01,\n            min_child_weight=2,\n            silent=-1,\n            verbose=-1,\n        )\n        \n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)], categorical_feature=cat_feats,\n                eval_metric='auc', verbose=100, early_stopping_rounds=40  #30\n               )\n        \n        vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n\n        oof_preds[val_idx] = clf.predict_proba(val_x, num_iteration=clf.best_iteration_)[:, 1]\n        minmin= min(oof_preds[val_idx])\n        maxmax= max(oof_preds[val_idx])\n        dataset_blend_train[val_idx,0]= vfunc(oof_preds[val_idx])\n        sub_preds = clf.predict_proba(X_test, num_iteration=clf.best_iteration_)[:, 1]\n        #dataset_blend_test_0[:,n_fold] = sub_preds\n        minmin= min(sub_preds)\n        maxmax= max(sub_preds)\n        dataset_blend_test_0[:,n_fold] = vfunc(sub_preds)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    print('Full AUC score %.6f' % roc_auc_score(y_train, oof_preds)) \n    return dataset_blend_train,dataset_blend_test_0","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:35.160703Z","iopub.execute_input":"2021-11-18T05:24:35.160947Z","iopub.status.idle":"2021-11-18T05:24:35.176874Z","shell.execute_reply.started":"2021-11-18T05:24:35.160919Z","shell.execute_reply":"2021-11-18T05:24:35.175826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train,dataset_blend_test_0 = lgb_model_1(x_train, y_train, x_test)\ndataset_blend_test[:,0]= dataset_blend_test_0.mean(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:24:35.178542Z","iopub.execute_input":"2021-11-18T05:24:35.178957Z","iopub.status.idle":"2021-11-18T05:31:56.142133Z","shell.execute_reply.started":"2021-11-18T05:24:35.178912Z","shell.execute_reply":"2021-11-18T05:31:56.141273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame([])\nsubmit['id']=test['loan_id']\nsubmit['isDefault']=dataset_blend_test[:,0]\nsubmit.to_csv('lgb_submit.csv', index=False)\n#状态 / 得分 \n#0.89358266770 ","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:31:56.143722Z","iopub.execute_input":"2021-11-18T05:31:56.144589Z","iopub.status.idle":"2021-11-18T05:31:56.179196Z","shell.execute_reply.started":"2021-11-18T05:31:56.144542Z","shell.execute_reply":"2021-11-18T05:31:56.178217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gg","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:31:56.180718Z","iopub.execute_input":"2021-11-18T05:31:56.180947Z","iopub.status.idle":"2021-11-18T05:31:56.359948Z","shell.execute_reply.started":"2021-11-18T05:31:56.180919Z","shell.execute_reply":"2021-11-18T05:31:56.358353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"markdown","source":"## CatBoost","metadata":{}},{"cell_type":"markdown","source":"### Step 1.","metadata":{}},{"cell_type":"code","source":"SEED = 7777","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:33:35.127462Z","iopub.execute_input":"2021-11-18T05:33:35.127878Z","iopub.status.idle":"2021-11-18T05:33:35.132336Z","shell.execute_reply.started":"2021-11-18T05:33:35.127842Z","shell.execute_reply":"2021-11-18T05:33:35.131778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport re\nimport pandas as pd\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom dateutil.relativedelta import relativedelta\ntrain_data = pd.read_csv('./train_public.csv')\nsubmit_example = pd.read_csv('../input/ccf2021bankdefault/submit_example.csv')\ntest_public = pd.read_csv('./test_public.csv')\ntrain_inte = pd.read_csv('./train_internet.csv')\n\ndef train_model(data_, test_, y_, folds_):\n    oof_preds = np.zeros(data_.shape[0])\n    sub_preds = np.zeros(test_.shape[0])\n    feature_importance_df = pd.DataFrame()\n    #feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault','policy_code','del_in_18month'] ]\n    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n        cat_feats={'industry','employer_type'}\n        clf = CatBoostClassifier(\n        n_estimators=5000,\n        learning_rate=0.07,\n        #num_leaves=2**5,\n        colsample_bylevel=.55,\n        subsample=.8,\n#             depth=11,\n#             max_bin=250,\n#             reg_alpha=.3,\n#             reg_lambda=.3,\n#             min_split_gain=.01,\n#             min_child_weight=2,\n        logging_level='Verbose',loss_function='Logloss',eval_metric='AUC'\n)\n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)], use_best_model=True,early_stopping_rounds=40, verbose=100#\n               )\n        oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n        sub_preds += clf.predict_proba(test_[feats])[:, 1] / folds_.n_splits\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n        gc.collect()  \n    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n    \n    test_['isDefault'] = sub_preds\n\n    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\ndef display_importances(feature_importance_df_):\n    # Plot feature importances\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(8,10))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances.png')\ndef workYearDIc(x):\n    if str(x)=='nan':\n        return -1\n    x = x.replace('< 1','0')\n    return int(re.search('(\\d+)', x).group())\ndef findDig(val):\n    fd = re.search('(\\d+-)', val)\n    if fd is None:\n        return '1-'+val\n    return val + '-01'\nclass_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n}\n#public+inte 双表\n\n#work_year\ntrain_data['work_year'] = train_data['work_year'].map(workYearDIc)\ntest_public['work_year'] = test_public['work_year'].map(workYearDIc)\ntrain_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n\n#class\ntrain_data['class'] = train_data['class'].map(class_dict)\ntest_public['class'] = test_public['class'].map(class_dict)\ntrain_inte['class'] = train_inte['class'].map(class_dict)\n\n#earlies_credit_mon\ntrain_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\ntest_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\ntrain_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'].map(findDig))\n\n#earlies_credit_mon\ntimeMax = pd.to_datetime('1-Dec-21')\ntrain_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \ntest_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\ntrain_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n\n#issue_date\ntrain_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\ntest_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\ntrain_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n\ntrain_data['issue_date_month'] = train_data['issue_date'].dt.month\ntest_public['issue_date_month'] = test_public['issue_date'].dt.month\ntrain_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n\ntrain_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\ntest_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\ntrain_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n\ntrain_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\ntest_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\ntrain_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n\ntrain_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\ntest_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\ntrain_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n\n#特征构造\n\n#月收入\n# train_data['monthly_income'] = train_data['monthly_payment'] / train_data['debt_loan_ratio'] *100\n# test_public['monthly_income'] = test_public['monthly_payment'] / test_public['debt_loan_ratio'] *100\n# train_inte['monthly_income'] = train_inte['monthly_payment'] / train_inte['debt_loan_ratio'] *100\n\ntrain_data['post_code_to_mean_interst'] = train_data.groupby(['post_code'])['interest'].transform('mean')\ntest_public['post_code_to_mean_interst'] = test_public.groupby(['post_code'])['interest'].transform('mean')\ntrain_inte['post_code_to_mean_interst'] = train_inte.groupby(['post_code'])['interest'].transform('mean')\n\ntrain_data['post_code_cnt'] = train_data['post_code'].map(train_data['post_code'].value_counts())\ntrain_data.loc[train_data['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntest_public['post_code_cnt'] = test_public['post_code'].map(test_public['post_code'].value_counts())\ntest_public.loc[test_public['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntrain_inte['post_code_cnt'] = train_inte['post_code'].map(train_inte['post_code'].value_counts())\ntrain_inte.loc[train_inte['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\n\n\ndel train_data['post_code_cnt']\ndel test_public['post_code_cnt']\ndel train_inte['post_code_cnt']\n#类别型特征编码\nfrom sklearn.preprocessing import LabelEncoder\ncat_cols = ['employer_type', 'industry']\nfor col in cat_cols:\n    lbl = LabelEncoder().fit(train_data[col])\n    train_data[col] = lbl.transform(train_data[col])\n    test_public[col] = lbl.transform(test_public[col])\n    train_inte[col] = lbl.transform(train_inte[col])\ncol_to_drop = ['issue_date', 'earlies_credit_mon']\ntrain_data = train_data.drop(col_to_drop, axis=1)\ntest_public = test_public.drop(col_to_drop, axis=1 )\ntrain_inte = train_inte.drop(col_to_drop, axis=1 )\n#取train_data和train_inte特征交集，train_inte将不包含的特征使用nan值填补\ntr_cols = set(train_data.columns)\nsame_col = list(tr_cols.intersection(set(train_inte.columns)))\ntrain_inteSame = train_inte[same_col].copy()\nInte_add_cos = list(tr_cols.difference(set(same_col)))\nfor col in Inte_add_cos:\n    train_inteSame[col] = np.nan\n#伪标签学习      以train_data特征训练模型->预测train_inteSame\ny = train_data['isDefault']\ntrain_data.shape\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:35:39.458862Z","iopub.execute_input":"2021-11-18T05:35:39.459201Z","iopub.status.idle":"2021-11-18T05:35:57.426777Z","shell.execute_reply.started":"2021-11-18T05:35:39.459168Z","shell.execute_reply":"2021-11-18T05:35:57.425851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#roc_auc_score\nfrom sklearn.metrics import roc_auc_score\nIntePre['isDef'] = train_inte['is_default']\nroc_auc_score(IntePre['isDef'],IntePre.isDefault)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:44:53.911091Z","iopub.execute_input":"2021-11-18T05:44:53.91148Z","iopub.status.idle":"2021-11-18T05:44:54.215662Z","shell.execute_reply.started":"2021-11-18T05:44:53.911447Z","shell.execute_reply":"2021-11-18T05:44:54.214762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 2.","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# train_data = pd.read_csv('./train_public.csv')\n# test_data = pd.read_csv('./test_public.csv')\n# sub=pd.read_csv(\"nn2.csv\")\n# sub=sub.rename(columns={'id': 'loan_id'})\n# thr = sub['isDefault'].quantile(0.9)\n# sub.loc[sub['isDefault']>thr,'isDefault'] = 1\n# sub.loc[sub['isDefault']<0.5,'isDefault'] = 0\n# nw_sub=sub[(sub['isDefault']==1)|(sub['isDefault']==0)]\n# nw_test_data=test_data.merge(nw_sub,on='loan_id',how='inner')\n# nw_train_data = pd.concat([train_data,nw_test_data]).reset_index(drop=True)\n# nw_train_data.to_csv(\"./nw_train_public.csv\",index=0)\n## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\nInteId = IntePre.loc[IntePre.isDefault<0.05, 'loan_id'].tolist()\n#新增来源域分类特征\ntrain_data['dataSourse'] = 1\ntest_public['dataSourse'] = 1\ntrain_inteSame['dataSourse'] = 0\ntrain_inteSame['isDefault'] = train_inte['is_default']\nuse_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\n#连接表\ndata = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:44:56.620573Z","iopub.execute_input":"2021-11-18T05:44:56.620909Z","iopub.status.idle":"2021-11-18T05:44:56.880192Z","shell.execute_reply.started":"2021-11-18T05:44:56.62088Z","shell.execute_reply":"2021-11-18T05:44:56.879317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IntePre.isDefault\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of Default values IntePre\")\nsns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\nplt.legend();plt.show()\ntrain = data[data['isDefault'].notna()]\ntest  = data[data['isDefault'].isna()]\n\ndel data\ndel train_data,test_public","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:44:58.862424Z","iopub.execute_input":"2021-11-18T05:44:58.86274Z","iopub.status.idle":"2021-11-18T05:45:03.40415Z","shell.execute_reply.started":"2021-11-18T05:44:58.862707Z","shell.execute_reply":"2021-11-18T05:45:03.403429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train['isDefault']\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, test_preds, importances = train_model(train, test, y, folds)\ntest_preds.rename({'loan_id': 'id'}, axis=1)[['id', 'isDefault']].to_csv('./nn3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:45:03.40582Z","iopub.execute_input":"2021-11-18T05:45:03.406953Z","iopub.status.idle":"2021-11-18T05:45:20.803514Z","shell.execute_reply.started":"2021-11-18T05:45:03.40691Z","shell.execute_reply":"2021-11-18T05:45:20.801572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Step 3.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ntrain_data = pd.read_csv('./train_public.csv')\ntest_data = pd.read_csv('./test_public.csv')\nsub=pd.read_csv(\"nn3.csv\")\nsub=sub.rename(columns={'id': 'loan_id'})","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:45:20.805597Z","iopub.execute_input":"2021-11-18T05:45:20.805841Z","iopub.status.idle":"2021-11-18T05:45:20.897284Z","shell.execute_reply.started":"2021-11-18T05:45:20.805815Z","shell.execute_reply":"2021-11-18T05:45:20.896228Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.loc[sub['isDefault']<0.5,'isDefault'] = 0\nnw_sub=sub[(sub['isDefault']==0)]\nnw_test_data=test_data.merge(nw_sub,on='loan_id',how='inner')\nnw_train_data = pd.concat([train_data,nw_test_data]).reset_index(drop=True)\nnw_train_data.to_csv(\"./nw_train_public.csv\",index=0)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:45:20.899032Z","iopub.execute_input":"2021-11-18T05:45:20.899503Z","iopub.status.idle":"2021-11-18T05:45:21.325518Z","shell.execute_reply.started":"2021-11-18T05:45:20.899455Z","shell.execute_reply":"2021-11-18T05:45:21.324811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport re\nimport pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve, roc_curve, average_precision_score\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom dateutil.relativedelta import relativedelta\ntrain_data = pd.read_csv('./nw_train_public.csv')\nsubmit_example = pd.read_csv('../input/ccf2021bankdefault/submit_example.csv')\ntest_public = pd.read_csv('./test_public.csv')\ntrain_inte = pd.read_csv('./train_internet.csv')\n\npd.set_option('max_columns', None)\npd.set_option('max_rows', 200)\npd.set_option('float_format', lambda x: '%.3f' % x)\ndef train_model(data_, test_, y_, folds_):\n    oof_preds = np.zeros(data_.shape[0])\n    sub_preds = np.zeros(test_.shape[0])\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in data_.columns if f not in ['loan_id', 'user_id', 'isDefault'] ]\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(data_)):\n        trn_x, trn_y = data_[feats].iloc[trn_idx], y_.iloc[trn_idx]\n        val_x, val_y = data_[feats].iloc[val_idx], y_.iloc[val_idx]\n        clf = CatBoostClassifier(\n        n_estimators=5000,\n        learning_rate=0.07,\n        #num_leaves=2**5,\n        colsample_bylevel=.55,\n        subsample=.8,\n#             depth=11,\n#             max_bin=250,\n#             reg_alpha=.3,\n#             reg_lambda=.3,\n#             min_split_gain=.01,\n#             min_child_weight=2,\n        logging_level='Verbose',loss_function='Logloss',eval_metric='AUC'\n)\n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)], use_best_model=True,early_stopping_rounds=40, verbose=100#\n               )\n        oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n        sub_preds += clf.predict_proba(test_[feats])[:, 1] / folds_.n_splits\n        \n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = feats\n        fold_importance_df[\"importance\"] = clf.feature_importances_\n        fold_importance_df[\"fold\"] = n_fold + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n        del clf, trn_x, trn_y, val_x, val_y\n        gc.collect()\n        \n    print('Full AUC score %.6f' % roc_auc_score(y, oof_preds)) \n    \n    test_['isDefault'] = sub_preds\n\n    return oof_preds, test_[['loan_id', 'isDefault']], feature_importance_df\ndef workYearDIc(x):\n    if str(x)=='nan':\n        return -1\n    x = x.replace('< 1','0')\n    return int(re.search('(\\d+)', x).group())\n\ndef findDig(val):\n    fd = re.search('(\\d+-)', val)\n    if fd is None:\n        return '1-'+val\n    return val + '-01'\n\n\nclass_dict = {\n    'A': 1,\n    'B': 2,\n    'C': 3,\n    'D': 4,\n    'E': 5,\n    'F': 6,\n    'G': 7,\n}\n\n#public+inte 双表\n\n#work_year\ntrain_data['work_year'] = train_data['work_year'].map(workYearDIc)\ntest_public['work_year'] = test_public['work_year'].map(workYearDIc)\ntrain_inte['work_year'] = train_inte['work_year'].map(workYearDIc)\n\n#class\ntrain_data['class'] = train_data['class'].map(class_dict)\ntest_public['class'] = test_public['class'].map(class_dict)\ntrain_inte['class'] = train_inte['class'].map(class_dict)\n\n#earlies_credit_mon\ntrain_data['earlies_credit_mon'] = pd.to_datetime(train_data['earlies_credit_mon'].map(findDig))\ntest_public['earlies_credit_mon'] = pd.to_datetime(test_public['earlies_credit_mon'].map(findDig))\ntrain_inte['earlies_credit_mon'] = pd.to_datetime(train_inte['earlies_credit_mon'].map(findDig))\n\n#earlies_credit_mon\ntimeMax = pd.to_datetime('1-Dec-21')\ntrain_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_data.loc[ train_data['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+  pd.offsets.DateOffset(years=-100)  \ntest_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = test_public.loc[ test_public['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\ntrain_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ] = train_inte.loc[ train_inte['earlies_credit_mon']>timeMax,'earlies_credit_mon' ]+ pd.offsets.DateOffset(years=-100)\n\n#issue_date\ntrain_data['issue_date'] = pd.to_datetime(train_data['issue_date'])\ntest_public['issue_date'] = pd.to_datetime(test_public['issue_date'])\ntrain_inte['issue_date'] = pd.to_datetime(train_inte['issue_date'])\n\ntrain_data['issue_date_month'] = train_data['issue_date'].dt.month\ntest_public['issue_date_month'] = test_public['issue_date'].dt.month\ntrain_inte['issue_date_month'] = train_inte['issue_date'].dt.month\n\ntrain_data['issue_date_dayofweek'] = train_data['issue_date'].dt.dayofweek\ntest_public['issue_date_dayofweek'] = test_public['issue_date'].dt.dayofweek\ntrain_inte['issue_date_dayofweek'] = train_inte['issue_date'].dt.dayofweek\n\ntrain_data['earliesCreditMon'] = train_data['earlies_credit_mon'].dt.month\ntest_public['earliesCreditMon'] = test_public['earlies_credit_mon'].dt.month\ntrain_inte['earliesCreditMon'] = train_inte['earlies_credit_mon'].dt.month\n\ntrain_data['earliesCreditYear'] = train_data['earlies_credit_mon'].dt.year\ntest_public['earliesCreditYear'] = test_public['earlies_credit_mon'].dt.year\ntrain_inte['earliesCreditYear'] = train_inte['earlies_credit_mon'].dt.year\n\n#特征构造\n\n#月收入\n# train_data['monthly_income'] = train_data['monthly_payment'] / train_data['debt_loan_ratio'] *100\n# test_public['monthly_income'] = test_public['monthly_payment'] / test_public['debt_loan_ratio'] *100\n# train_inte['monthly_income'] = train_inte['monthly_payment'] / train_inte['debt_loan_ratio'] *100\n\ntrain_data['post_code_to_mean_interst'] = train_data.groupby(['post_code'])['interest'].transform('mean')\ntest_public['post_code_to_mean_interst'] = test_public.groupby(['post_code'])['interest'].transform('mean')\ntrain_inte['post_code_to_mean_interst'] = train_inte.groupby(['post_code'])['interest'].transform('mean')\n\ntrain_data['post_code_cnt'] = train_data['post_code'].map(train_data['post_code'].value_counts())\ntrain_data.loc[train_data['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntest_public['post_code_cnt'] = test_public['post_code'].map(test_public['post_code'].value_counts())\ntest_public.loc[test_public['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\ntrain_inte['post_code_cnt'] = train_inte['post_code'].map(train_inte['post_code'].value_counts())\ntrain_inte.loc[train_inte['post_code_cnt']<=5,'post_code_to_mean_interst'] = np.nan\n\ndel train_data['post_code_cnt']\ndel test_public['post_code_cnt']\ndel train_inte['post_code_cnt']\n\nfrom sklearn.preprocessing import LabelEncoder\nfor col in cat_cols:\n    cat_cols = ['employer_type', 'industry']\n    lbl = LabelEncoder().fit(train_data[col])\n    train_data[col] = lbl.transform(train_data[col])\n    test_public[col] = lbl.transform(test_public[col])\n    \n    #Internet处理\n    train_inte[col] = lbl.transform(train_inte[col])\n    \ncol_to_drop = ['issue_date', 'earlies_credit_mon']\ntrain_data = train_data.drop(col_to_drop, axis=1)\ntest_public = test_public.drop(col_to_drop, axis=1 )\ntrain_inte = train_inte.drop(col_to_drop, axis=1 )\n\ntr_cols = set(train_data.columns)\nsame_col = list(tr_cols.intersection(set(train_inte.columns)))\ntrain_inteSame = train_inte[same_col].copy()\n\nInte_add_cos = list(tr_cols.difference(set(same_col)))\nfor col in Inte_add_cos:\n    train_inteSame[col] = np.nan\n\ny = train_data['isDefault']\nfolds = KFold(n_splits=5, shuffle=True, random_state=SEED)\noof_preds, IntePre, importances = train_model(train_data, train_inteSame, y, folds)\n\nIntePre['isDef'] = train_inte['is_default']\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(IntePre['isDef'],IntePre.isDefault)\n## 选择阈值0.05，从internet表中提取预测小于该概率的样本，并对不同来源的样本赋予来源值\nInteId = IntePre.loc[IntePre.isDefault<0.5, 'loan_id'].tolist()\n\ntrain_data['dataSourse'] = 1\ntest_public['dataSourse'] = 1\ntrain_inteSame['dataSourse'] = 0\ntrain_inteSame['isDefault'] = train_inte['is_default']\nuse_te = train_inteSame[train_inteSame.loan_id.isin( InteId )].copy()\ndata = pd.concat([ train_data,test_public,use_te]).reset_index(drop=True)\n\n####****新增数据######\n# interest#当前贷款利率\n# debt_loan_ratio#债务收入比\n\nfor method in ['mean','std','sum','median']:\n    for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n        data[f'label_{method}_'+str(col)] = data.groupby(col)['isDefault'].transform(method)\n        \n# for method in ['mean', 'max', 'min', 'std','sum','median']:\n#     for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n#         data[f'interest_{method}_'+str(col)] = data.groupby(col)['known_outstanding_loan'].transform(method)\n# for method in ['mean', 'max', 'min', 'std','sum','median']:\n#     for col in ['employer_type', 'industry','issue_date_month','issue_date_dayofweek','earliesCreditMon','earliesCreditYear','region']:\n#         data[f'debt_loan_ratio_{method}_'+str(col)] = data.groupby(col)['debt_loan_ratio'].transform(method)\n\n\n# data = data.replace([-np.inf,np.inf],np.NaN)\n# Kdf = data[['known_outstanding_loan','debt_loan_ratio','monthly_payment','f2','total_loan','f4','f3','interest']]\n# Kdf = Kdf.fillna(0)\n# kmeans = KMeans(3,random_state=2021)\n# kmeans.fit(Kdf)\n# identified_clusters = kmeans.predict(Kdf)\n# data['k_cluster'] = identified_clusters\n\n# for method in ['mean', 'max', 'min', 'std']:\n#     for col in ['known_outstanding_loan','debt_loan_ratio','monthly_payment','f2','total_loan','f4','f3','interest']:\n#         data[f'k_cluster_{method}_'+str(col)] = data.groupby('k_cluster')[col].transform(method)\n        \nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of Default values IntePre\")\nsns.distplot(IntePre['isDefault'],color=\"black\", kde=True,bins=120, label='train_data')\n# sns.distplot(train_inte[col],color=\"red\", kde=True,bins=120, label='train_inte')\nplt.legend();plt.show()\ntrain = data[data['isDefault'].notna()]\ntest  = data[data['isDefault'].isna()]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:45:59.011432Z","iopub.execute_input":"2021-11-18T05:45:59.011815Z","iopub.status.idle":"2021-11-18T05:46:29.204581Z","shell.execute_reply.started":"2021-11-18T05:45:59.011771Z","shell.execute_reply":"2021-11-18T05:46:29.20383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb_model_0\nfeats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault']]\n#lgb_model_1\n#feats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault','f1']]\nx_train = train[feats]\ny_train = train['isDefault']\nx_test = test[feats]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:46:29.206656Z","iopub.execute_input":"2021-11-18T05:46:29.207755Z","iopub.status.idle":"2021-11-18T05:46:29.45843Z","shell.execute_reply.started":"2021-11-18T05:46:29.207704Z","shell.execute_reply":"2021-11-18T05:46:29.457038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train = np.zeros((x_train.shape[0], 4))\ndataset_blend_test = np.zeros((x_test.shape[0], 4))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:46:29.460829Z","iopub.execute_input":"2021-11-18T05:46:29.461238Z","iopub.status.idle":"2021-11-18T05:46:29.478446Z","shell.execute_reply.started":"2021-11-18T05:46:29.461195Z","shell.execute_reply":"2021-11-18T05:46:29.477279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cat_model_0(X_train, y_train, X_test, y_test=None):\n    #X_train = pd.DataFrame(X_train)\n    #X_test = pd.DataFrame(X_test)\n    folds_ = KFold(n_splits=5, shuffle=True, random_state=SEED)\n    oof_preds = np.zeros(X_train.shape[0])\n    sub_preds = np.zeros(X_test.shape[0])\n    cat_feats={'industry','employer_type'}\n    dataset_blend_test_0 = np.zeros((X_test.shape[0], 5))\n    for n_fold, (trn_idx, val_idx) in enumerate(folds_.split(X_train)):\n        trn_x, trn_y = X_train.iloc[trn_idx], y_train.iloc[trn_idx]\n        val_x, val_y = X_train.iloc[val_idx], y_train.iloc[val_idx]\n        \n        clf = CatBoostClassifier(\n        n_estimators=5000,\n        learning_rate=0.07,\n        #num_leaves=2**5,\n        colsample_bylevel=.55,\n        subsample=.8,\n#             depth=11,\n#             max_bin=250,\n#             reg_alpha=.3,\n#             reg_lambda=.3,\n#             min_split_gain=.01,\n#             min_child_weight=2,\n        logging_level='Verbose',loss_function='Logloss',eval_metric='AUC'\n)\n        clf.fit(trn_x, trn_y, \n                eval_set= [(trn_x, trn_y), (val_x, val_y)], use_best_model=True,early_stopping_rounds=40, verbose=100#\n               )\n\n        \n        vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n\n        oof_preds[val_idx] = clf.predict_proba(val_x)[:, 1]\n        minmin= min(oof_preds[val_idx])\n        maxmax= max(oof_preds[val_idx])\n        dataset_blend_train[val_idx,0]= vfunc(oof_preds[val_idx])\n        sub_preds = clf.predict_proba(X_test)[:, 1]\n        #dataset_blend_test_0[:,n_fold] = sub_preds\n        minmin= min(sub_preds)\n        maxmax= max(sub_preds)\n        dataset_blend_test_0[:,n_fold] = vfunc(sub_preds)\n        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    print('Full AUC score %.6f' % roc_auc_score(y_train, oof_preds)) \n    return dataset_blend_train,dataset_blend_test_0","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:46:29.481095Z","iopub.execute_input":"2021-11-18T05:46:29.481607Z","iopub.status.idle":"2021-11-18T05:46:29.495399Z","shell.execute_reply.started":"2021-11-18T05:46:29.481572Z","shell.execute_reply":"2021-11-18T05:46:29.494621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train,dataset_blend_test_0 = cat_model_0(x_train, y_train, x_test)\ndataset_blend_test[:,0]= dataset_blend_test_0.mean(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:46:29.496666Z","iopub.execute_input":"2021-11-18T05:46:29.497219Z","iopub.status.idle":"2021-11-18T06:05:37.350292Z","shell.execute_reply.started":"2021-11-18T05:46:29.497189Z","shell.execute_reply":"2021-11-18T06:05:37.348987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['isDefault'] = dataset_blend_test[:,0]\nthr1 = test['isDefault'].quantile(0.9)\nthr2 = test['isDefault'].quantile(0.1)\ntest['isDefault'][dataset_blend_test[:,0]>thr1]=1\ntest['isDefault'][dataset_blend_test[:,0]<thr2]=0\ntest.shape","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:05:37.352873Z","iopub.execute_input":"2021-11-18T06:05:37.353474Z","iopub.status.idle":"2021-11-18T06:05:37.369865Z","shell.execute_reply.started":"2021-11-18T06:05:37.353436Z","shell.execute_reply":"2021-11-18T06:05:37.369265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dat = test[(test['isDefault']==1) | (test['isDefault']==0)]\ntrain = pd.concat([train,new_dat])","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:05:37.371181Z","iopub.execute_input":"2021-11-18T06:05:37.371591Z","iopub.status.idle":"2021-11-18T06:05:37.659805Z","shell.execute_reply.started":"2021-11-18T06:05:37.371561Z","shell.execute_reply":"2021-11-18T06:05:37.658792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lgb_model_0\nfeats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault']]\n#lgb_model_1\n#feats =[f for f in train.columns if f not in ['loan_id', 'user_id', 'isDefault','f1']]\nx_train = train[feats]\ny_train = train['isDefault']\nx_test = test[feats]","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:05:37.661278Z","iopub.execute_input":"2021-11-18T06:05:37.661506Z","iopub.status.idle":"2021-11-18T06:05:38.06944Z","shell.execute_reply.started":"2021-11-18T06:05:37.66148Z","shell.execute_reply":"2021-11-18T06:05:38.068337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train = np.zeros((x_train.shape[0], 4))\ndataset_blend_test = np.zeros((x_test.shape[0], 4))","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:05:38.070922Z","iopub.execute_input":"2021-11-18T06:05:38.071146Z","iopub.status.idle":"2021-11-18T06:05:38.078531Z","shell.execute_reply.started":"2021-11-18T06:05:38.07112Z","shell.execute_reply":"2021-11-18T06:05:38.077636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset_blend_train,dataset_blend_test_0 = lgb_model_1(x_train, y_train, x_test)\ndataset_blend_test[:,0]= dataset_blend_test_0.mean(1)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:18:13.666191Z","iopub.execute_input":"2021-11-18T06:18:13.667072Z","iopub.status.idle":"2021-11-18T06:25:13.663286Z","shell.execute_reply.started":"2021-11-18T06:18:13.667018Z","shell.execute_reply":"2021-11-18T06:25:13.662589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = pd.DataFrame([])\nsubmit['id']=test['loan_id']\nsubmit['isDefault']=dataset_blend_test[:,0]\nsubmit.to_csv('cat_submit.csv', index=False)\n#状态 / 得分 \n#0.89358266770 ","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:27:33.598418Z","iopub.execute_input":"2021-11-18T06:27:33.599381Z","iopub.status.idle":"2021-11-18T06:27:33.633874Z","shell.execute_reply.started":"2021-11-18T06:27:33.599333Z","shell.execute_reply":"2021-11-18T06:27:33.632945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub1 = pd.read_csv('./lgb_submit.csv')\nsub2 = pd.read_csv('./cat_submit.csv')\nsub3 = pd.read_csv('../input/ccf2021bankdefault/sub897.csv')\nsub2.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:37:17.055189Z","iopub.execute_input":"2021-11-18T06:37:17.055793Z","iopub.status.idle":"2021-11-18T06:37:17.084269Z","shell.execute_reply.started":"2021-11-18T06:37:17.055752Z","shell.execute_reply":"2021-11-18T06:37:17.083478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub2.isDefault = (sub3.isDefault*0.6+sub1.isDefault*0.4)\nsub2.to_csv('final_sub.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:37:19.623832Z","iopub.execute_input":"2021-11-18T06:37:19.624739Z","iopub.status.idle":"2021-11-18T06:37:19.654389Z","shell.execute_reply.started":"2021-11-18T06:37:19.624666Z","shell.execute_reply":"2021-11-18T06:37:19.6535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(sub2.isDefault)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:48:15.1715Z","iopub.execute_input":"2021-11-18T06:48:15.172184Z","iopub.status.idle":"2021-11-18T06:48:15.547929Z","shell.execute_reply.started":"2021-11-18T06:48:15.172145Z","shell.execute_reply":"2021-11-18T06:48:15.547348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(sub3.isDefault)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T06:48:01.086191Z","iopub.execute_input":"2021-11-18T06:48:01.086545Z","iopub.status.idle":"2021-11-18T06:48:01.483596Z","shell.execute_reply.started":"2021-11-18T06:48:01.086496Z","shell.execute_reply":"2021-11-18T06:48:01.482699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb1_train = oof_preds\nlgb2_train = oof_preds2\nlgb1_test = dataset_blend_test[:,0]\nlgb2_test = test_preds2.isDefault\n#\ntrain_1 = np.zeros((oof_preds.shape[0],6))\ntest_1 = np.zeros((test.shape[0],6))\ntrain_1[:,0] = lgb1_train**2\ntest_1[:,0] = lgb1_test**2\ntrain_1[:,1] = np.exp(lgb1_train)\ntest_1[:,1] = np.exp(lgb1_test)\n\ntrain_1[:,2] = lgb2_train**2\ntest_1[:,2] = lgb2_test**2\ntrain_1[:,3] = np.exp(lgb2_train)\ntest_1[:,3] = np.exp(lgb2_test)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n#train_ = pd.DataFrame(train_1)\n#test_ = pd.DataFrame(test_1)\nclf = LogisticRegression()\n#clf = GradientBoostingClassifier(learning_rate=0.02, subsample=0.5, max_depth=6, n_estimators=30)\n#clf= RandomForestClassifier(n_estimators=50, n_jobs=-1, criterion='gini')\nclf.fit(train_1, y_train)\ny_emb = clf.predict_proba(test_1)[:, 1]\nvfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\nminmin= min(y_emb)\nmaxmax= max(y_emb)\ny_emb1=vfunc(y_emb)","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:31:56.373787Z","iopub.status.idle":"2021-11-18T05:31:56.374122Z","shell.execute_reply.started":"2021-11-18T05:31:56.373934Z","shell.execute_reply":"2021-11-18T05:31:56.373951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sub1 = pd.read_csv('baseline_catboost.csv')\n# sub2 = pd.read_csv('baseline_lgb.csv')\n# sub1['isDefault'] = (sub1['isDefault']+sub2['isDefault'])/2\n# sub1.to_csv('cat_lgb_sub.csv',index=False)\n","metadata":{"execution":{"iopub.status.busy":"2021-11-18T05:31:56.375369Z","iopub.status.idle":"2021-11-18T05:31:56.375732Z","shell.execute_reply.started":"2021-11-18T05:31:56.375538Z","shell.execute_reply":"2021-11-18T05:31:56.375559Z"},"trusted":true},"execution_count":null,"outputs":[]}]}